1. ownership head 사용
게임 종료후 실제 영역이 된 부분에 대해서 예측하는 헤드
caputture로 이긴 경우 어떻게 할거냐 -> 그냥 무시하고 영역으로 이긴것만저장? 
또는 caputre로 이긴 보드에 대해서는 가중치 낮게 주기



2. 보드 저장 표현 변경

수순 리스트 (action sequence)
1D action index들의 배열 (pass 포함)
이게 제일 확실하고 아주 작음
예: [12, 37, 81(pass), 5, ...]



3. buffer 중간 저장
용량이 꽤 클거같은데 굳이 필요할까? 근데 다 날라가기는 해서 다음부터는 처음 3개의 iteration에 대해서는 학습하지 않도록 하기


4. 지금 모델 사이즈는 적절한걸까? 
b6c96: Strong/Top Amateur
b10c128: Strong Professional
b15c192: Superhuman
b20c256: Superhuman
카타고 기준 위와 같은 데 여기서 난이도를 고려하면 파라미터수는 1/16으로도 충분하지 않을까? 
이미 (64/256)^2 = 1/16으로 작아지기는 했고 블록도 작은데 일단 해보기



5. 학습 보드를 보니까 실수가 좀 많이 나옴 -> 초반에도 즉사할 수 있는 게임이기 때문에 초반 포석에 temp나 디레클레 노이즈를 너무 높게 주면 안되지 않을까 싶음
그래서 1-> 0.6으로 수정


6. v_search, 와 reanalyze적용
실제 보상으로만 판단한는게 아니라 모델의 평가값을 일정비율로 포함시켜서 실제보상+모델예측을 state의 가치로 판단하는것
reanalyze : 이미 지나간 state에 대해 다시한번 평가 즉 개선된 모델의 예측을 이용하여 실제보상+모델예측의 신뢰도를 높임

